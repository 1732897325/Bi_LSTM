{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-23T11:58:58.946181Z",
     "start_time": "2023-10-23T11:58:50.743992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6938 - accuracy: 0.3333 - val_loss: 0.7040 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6893 - accuracy: 0.6667 - val_loss: 0.7147 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6847 - accuracy: 0.6667 - val_loss: 0.7259 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6800 - accuracy: 0.6667 - val_loss: 0.7382 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6751 - accuracy: 0.6667 - val_loss: 0.7518 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6697 - accuracy: 0.6667 - val_loss: 0.7671 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6638 - accuracy: 0.6667 - val_loss: 0.7845 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6572 - accuracy: 0.6667 - val_loss: 0.8045 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6499 - accuracy: 0.6667 - val_loss: 0.8277 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6417 - accuracy: 0.6667 - val_loss: 0.8548 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.55445606]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 准备数据\n",
    "texts = [\"I love this movie\", \"This movie is great\", \"I dislike this movie\", \"This movie is terrible\"]\n",
    "labels = [1, 1, 0, 0]\n",
    "\n",
    "# 文本预处理\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_seq_length = max([len(seq) for seq in sequences])\n",
    "\n",
    "# 序列填充\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)\n",
    "\n",
    "# 构建Bi-LSTM模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_seq_length))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(np.array(x_train), np.array(y_train), batch_size=16, epochs=10, validation_data=(np.array(x_test), np.array(y_test)))\n",
    "\n",
    "# 使用模型进行预测\n",
    "test_sequences = tokenizer.texts_to_sequences([\"This movie is amazing\"])\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
    "prediction = model.predict(np.array(test_data))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图中的节点数量： 1155\n",
      "图中的边数量： 458013\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 读取train_90.csv文件\n",
    "df = pd.read_csv('train_90.csv')\n",
    "\n",
    "# 读取边文件\n",
    "edges = pd.read_csv('edge_90.csv')\n",
    "\n",
    "# 创建一个空的有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 遍历每一行数据\n",
    "for i in range(len(df)):\n",
    "    geohash_id = df.loc[i, 'geohash_id']\n",
    "    date_id = df.loc[i, 'date_id']\n",
    "\n",
    "    # 添加节点\n",
    "    G.add_node(geohash_id)\n",
    "\n",
    "    # 添加节点属性\n",
    "    node_attrs = {f'F_{j+1}': df.loc[i, f'F_{j+1}'] for j in range(35)}\n",
    "    G.nodes[geohash_id].update(node_attrs)\n",
    "\n",
    "# 添加边到图中\n",
    "for index, row in edges.iterrows():\n",
    "    G.add_edge(row['geohash6_point1'], row['geohash6_point2'], weight_f1=row['F_1'], weight_f2=row['F_2'])\n",
    "\n",
    "# 添加节点属性\n",
    "active_index = {row.geohash_id: row.active_index for row in df.itertuples(index=False)}\n",
    "consume_index = {row.geohash_id: row.consume_index for row in df.itertuples(index=False)}\n",
    "nx.set_node_attributes(G, active_index, 'active_index')\n",
    "nx.set_node_attributes(G, consume_index, 'consume_index')\n",
    "\n",
    "# 打印图的节点和边数量\n",
    "print(\"图中的节点数量：\", G.number_of_nodes())\n",
    "print(\"图中的边数量：\", G.number_of_edges())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T09:30:44.874992Z",
     "start_time": "2023-10-25T09:29:05.946960Z"
    }
   },
   "id": "a8f6367e1bf4a3d0"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换向量成功\n"
     ]
    }
   ],
   "source": [
    "# 增加获取邻居节点的函数\n",
    "def get_neighbors(G, node):\n",
    "    return list(G.neighbors(node))\n",
    "\n",
    "# 创建空的特征对应值\n",
    "features_values = {}\n",
    "for node in G.nodes():\n",
    "    # 确认'F_1'到'F_35'在df中都存在对应的列，如无需要进行删除修改\n",
    "    # 这里我们只获取存在的值\n",
    "    features_values[node] = [G.nodes[node][f'F_{j+1}'] for j in range(35) if f'F_{j+1}' in G.nodes[node]]\n",
    "\n",
    "# 初始化空的序列列表\n",
    "sequences = []\n",
    "for node in G.nodes():\n",
    "    # 对于每个节点，得到它的邻居节点并将其与自身一同作为序列输入\n",
    "    sequences.append([node] + get_neighbors(G, node))\n",
    "\n",
    "# 使用序列训练word2vec模型\n",
    "model = Word2Vec(sequences, vector_size=35, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 获取每个节点的向量表示\n",
    "node_vectors = {}\n",
    "for node in G:\n",
    "    node_vectors[node] = model.wv[node]\n",
    "\n",
    "#用node_vectors作为预测模型的输入\n",
    "print(\"转换向量成功\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T12:00:26.845511Z",
     "start_time": "2023-10-25T12:00:25.969379Z"
    }
   },
   "id": "8629ab875fd47fe7"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/PycharmProjects/Bi_LSTM_v1/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1155])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/50], Loss: 0.2497\n",
      "Epoch [200/50], Loss: 0.2497\n",
      "Epoch [300/50], Loss: 0.2497\n",
      "Epoch [400/50], Loss: 0.2497\n",
      "Epoch [500/50], Loss: 0.2497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 60\u001B[0m\n\u001B[1;32m     58\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     59\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, y)   \u001B[38;5;66;03m# 计算损失\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m                \u001B[38;5;66;03m# 反向传播\u001B[39;00m\n\u001B[1;32m     61\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()               \u001B[38;5;66;03m# 更新权重\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (epoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/Bi_LSTM_v1/venv/lib/python3.9/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Bi_LSTM_v1/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 2 for bidirection\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim=0)  # 增加批处理维度\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])    # 取最后一个时间步\n",
    "        return out.squeeze(dim=0)   # 移除批处理维度\n",
    "\n",
    "\n",
    "\n",
    "# 转化栅格数据为tensor\n",
    "# 这里是一个假设的例子，根据你实际的node_vectors来生成输入数据\n",
    "# 获得向量和标签列表，确保顺序一致\n",
    "nodes = list(node_vectors.keys())\n",
    "\n",
    "labels = {node: random.randint(0, 1) for node in nodes}\n",
    "\n",
    "X_list = [node_vectors[node] for node in nodes]\n",
    "y_list = [labels[node] for node in nodes]  # 假设你有一个和nodes对应的标签字典labels\n",
    "\n",
    "# 转化数据为tensor\n",
    "X = torch.tensor(X_list, dtype=torch.float32)\n",
    "y = torch.tensor(y_list, dtype=torch.float32)\n",
    "\n",
    "# 初始化模型\n",
    "input_dim = len(node_vectors[nodes[0]])  #输入维度取决于序列的特征数目\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "output_dim = 1    # 输出维度取决于你的任务，例如二分类任务这里为1\n",
    "\n",
    "model = BiLSTM(input_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "# 设置损失函数和优化器\n",
    "criterion = torch.nn.MSELoss()    # 使用均方误差作为损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) ## 优化方式为Adam梯度下降方法，学习率为0.001\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(1000):    # 你设定的训练轮数\n",
    "    model.train()\n",
    "    outputs = model(X)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, y)   # 计算损失\n",
    "    loss.backward()                # 反向传播\n",
    "    optimizer.step()               # 更新权重\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 50, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T12:00:19.305457Z",
     "start_time": "2023-10-25T11:56:54.715667Z"
    }
   },
   "id": "6678ddf6bfb24607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6976152011b8fbd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
